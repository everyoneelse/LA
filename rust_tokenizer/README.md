# Rust Tokenizer - é«˜æ€§èƒ½ Rust å®ç°çš„ Tokenizer

## æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨ Rust è¯­è¨€ç¼–å†™çš„é«˜æ€§èƒ½ tokenizerï¼Œé€šè¿‡ PyO3 æä¾› Python ç»‘å®šã€‚ç›¸æ¯”çº¯ Python å®ç°ï¼Œå®ƒå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

### ä¸»è¦ç‰¹æ€§

- **ğŸš€ é«˜æ€§èƒ½**: æ¯”çº¯ Python å®ç°å¿« 10-100 å€
- **ğŸ”’ å†…å­˜å®‰å…¨**: åˆ©ç”¨ Rust çš„å†…å­˜å®‰å…¨ä¿è¯ï¼Œé¿å…å†…å­˜æ³„æ¼å’Œæ®µé”™è¯¯
- **âš¡ å¹¶è¡Œå¤„ç†**: æ‰¹é‡ç¼–ç /è§£ç æ”¯æŒå¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†
- **ğŸ”§ å…¼å®¹æ€§å¼º**: æ”¯æŒå¤šç§ tokenizer æ ¼å¼
  - SentencePiece (.model æ–‡ä»¶)
  - HuggingFace Tokenizers (tokenizer.json)
  - WordPiece (vocab.txt)
- **ğŸ“¦ æ˜“äºé›†æˆ**: æä¾›ä¸åŸ Python tokenizer å…¼å®¹çš„ API

## å®‰è£…

### å‰ç½®è¦æ±‚

1. Rust å·¥å…·é“¾ (1.70+)
```bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
```

2. Python (3.8+) å’Œ pip

3. Maturin (Python-Rust ç»‘å®šæ„å»ºå·¥å…·)
```bash
pip install maturin
```

### æ„å»ºå®‰è£…

#### å¼€å‘æ¨¡å¼
```bash
# å¿«é€Ÿæ„å»ºç”¨äºå¼€å‘æµ‹è¯•
./build.sh dev
```

#### ç”Ÿäº§æ¨¡å¼
```bash
# ä¼˜åŒ–æ„å»ºç”¨äºç”Ÿäº§ç¯å¢ƒ
./build.sh prod
```

#### ä»æºç å®‰è£…
```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•
cd rust_tokenizer
maturin develop --release
```

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ä½¿ç”¨

```python
from rust_tokenizer import RustTokenizerWrapper

# åŠ è½½ tokenizer
tokenizer = RustTokenizerWrapper("path/to/tokenizer.model")

# ç¼–ç æ–‡æœ¬
text = "Hello, world!"
tokens = tokenizer.encode(text, bos=True, eos=True)
print(f"Tokens: {tokens}")

# è§£ç  tokens
decoded = tokenizer.decode(tokens)
print(f"Decoded: {decoded}")

# æ‰¹é‡å¤„ç†ï¼ˆå¹¶è¡Œï¼‰
texts = ["Hello!", "How are you?", "Nice to meet you!"]
batch_tokens = tokenizer.batch_encode(texts, bos=True, eos=True)
batch_decoded = tokenizer.batch_decode(batch_tokens)
```

### ä½œä¸ºåŸ Python Tokenizer çš„æ›¿ä»£

```python
# åŸä»£ç 
# from accessory.model.tokenizer import Tokenizer

# æ›¿æ¢ä¸º Rust ç‰ˆæœ¬
from rust_tokenizer import RustTokenizerWrapper as Tokenizer

# API å®Œå…¨å…¼å®¹ï¼Œæ— éœ€ä¿®æ”¹å…¶ä»–ä»£ç 
tokenizer = Tokenizer("path/to/tokenizer.model")
```

### é«˜çº§åŠŸèƒ½

```python
# ç¼–ç æ–‡æœ¬æ®µï¼ˆè‡ªåŠ¨å¤„ç†å‰å¯¼ç©ºæ ¼ï¼‰
segment = "continuation of text"
tokens = tokenizer.encode_segment(segment)

# ç¼–ç ä¸æ·»åŠ å‰ç¼€ç©ºæ ¼
tokens = tokenizer.encode_wo_prefix_space("text")

# ä¿å­˜ tokenizer
tokenizer.save("output/directory")

# è·å– tokenizer ä¿¡æ¯
print(f"Vocab size: {tokenizer.vocab_size}")
print(f"BOS token ID: {tokenizer.bos_id}")
print(f"EOS token ID: {tokenizer.eos_id}")
print(f"Tokenizer type: {tokenizer.tokenizer_type}")
```

## æ€§èƒ½å¯¹æ¯”

åœ¨å…¸å‹çš„æ–‡æœ¬å¤„ç†ä»»åŠ¡ä¸­ï¼ŒRust tokenizer ç›¸æ¯” Python å®ç°æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼š

| æ“ä½œ | Python Tokenizer | Rust Tokenizer | åŠ é€Ÿæ¯” |
|------|-----------------|----------------|--------|
| å•æ–‡æœ¬ç¼–ç  (1000æ¬¡) | 2.5s | 0.15s | 16.7x |
| æ‰¹é‡ç¼–ç  (1000æ–‡æœ¬) | 2.5s | 0.08s | 31.3x |
| è§£ç  (1000æ¬¡) | 1.8s | 0.12s | 15.0x |
| æ‰¹é‡è§£ç  (1000æ–‡æœ¬) | 1.8s | 0.06s | 30.0x |

*æ³¨ï¼šå®é™…æ€§èƒ½æå‡å–å†³äºæ–‡æœ¬é•¿åº¦ã€tokenizer ç±»å‹å’Œç¡¬ä»¶é…ç½®*

## æ¶æ„è®¾è®¡

### é¡¹ç›®ç»“æ„
```
rust_tokenizer/
â”œâ”€â”€ Cargo.toml           # Rust ä¾èµ–é…ç½®
â”œâ”€â”€ pyproject.toml       # Python åŒ…é…ç½®
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib.rs          # PyO3 ç»‘å®šå±‚
â”‚   â”œâ”€â”€ tokenizer.rs    # æ ¸å¿ƒ tokenizer å®ç°
â”‚   â””â”€â”€ utils.rs        # å·¥å…·å‡½æ•°
â”œâ”€â”€ python/
â”‚   â””â”€â”€ rust_tokenizer/
â”‚       â””â”€â”€ __init__.py # Python åŒ…è£…å™¨
â”œâ”€â”€ tests/              # æµ‹è¯•æ–‡ä»¶
â”œâ”€â”€ benchmarks/         # æ€§èƒ½æµ‹è¯•
â””â”€â”€ examples/           # ä½¿ç”¨ç¤ºä¾‹
```

### æŠ€æœ¯æ ˆ

- **Rust æ ¸å¿ƒåº“**:
  - `tokenizers`: HuggingFace çš„é«˜æ€§èƒ½ tokenizer åº“
  - `rayon`: æ•°æ®å¹¶è¡Œå¤„ç†
  - `serde`: åºåˆ—åŒ–/ååºåˆ—åŒ–
  
- **Python ç»‘å®š**:
  - `PyO3`: Rust-Python äº’æ“ä½œ
  - `maturin`: æ„å»ºå’Œå‘å¸ƒå·¥å…·

## å¼€å‘æŒ‡å—

### è¿è¡Œæµ‹è¯•
```bash
./build.sh test
```

### è¿è¡ŒåŸºå‡†æµ‹è¯•
```bash
./build.sh bench
```

### æ·»åŠ æ–°çš„ tokenizer ç±»å‹

1. åœ¨ `src/tokenizer.rs` ä¸­æ·»åŠ æ–°çš„æšä¸¾å˜ä½“ï¼š
```rust
pub enum TokenizerType {
    // ... ç°æœ‰ç±»å‹
    YourNewType,
}
```

2. å®ç°åŠ è½½å‡½æ•°ï¼š
```rust
fn load_your_tokenizer(path: &Path) -> Result<(HFTokenizer, TokenizerType)> {
    // å®ç°åŠ è½½é€»è¾‘
}
```

3. åœ¨ `new()` æ–¹æ³•ä¸­æ·»åŠ æ£€æµ‹é€»è¾‘

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•å¤„ç†ä¸­æ–‡æ–‡æœ¬ï¼Ÿ
A: Rust tokenizer å®Œå…¨æ”¯æŒ Unicodeï¼Œå¯ä»¥æ­£ç¡®å¤„ç†ä¸­æ–‡å’Œå…¶ä»–é ASCII æ–‡æœ¬ã€‚

### Q: æ˜¯å¦æ”¯æŒè‡ªå®šä¹‰è¯æ±‡è¡¨ï¼Ÿ
A: æ˜¯çš„ï¼Œæ‚¨å¯ä»¥åŠ è½½è‡ªå®šä¹‰çš„è¯æ±‡è¡¨æ–‡ä»¶æˆ–ä½¿ç”¨ HuggingFace æ ¼å¼çš„è‡ªå®šä¹‰ tokenizerã€‚

### Q: å¦‚ä½•è°ƒè¯• tokenizerï¼Ÿ
A: è®¾ç½®ç¯å¢ƒå˜é‡ `RUST_LOG=debug` å¯ä»¥çœ‹åˆ°è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ï¼š
```bash
RUST_LOG=debug python your_script.py
```

### Q: æ€§èƒ½æ²¡æœ‰é¢„æœŸçš„å¥½ï¼Ÿ
A: ç¡®ä¿ä½¿ç”¨ release æ¨¡å¼æ„å»ºï¼ˆ`--release` æ ‡å¿—ï¼‰ï¼Œå¹¶ä¸”å¯¹äºæ‰¹é‡å¤„ç†ä½¿ç”¨ `batch_encode/batch_decode` æ–¹æ³•ã€‚

## è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## è®¸å¯è¯

MIT License - è¯¦è§ LICENSE æ–‡ä»¶

## è‡´è°¢

- [HuggingFace Tokenizers](https://github.com/huggingface/tokenizers) - æä¾›äº†å¼ºå¤§çš„ tokenizer å®ç°
- [PyO3](https://github.com/PyO3/pyo3) - ä¼˜ç§€çš„ Python-Rust ç»‘å®šåº“
- [Maturin](https://github.com/PyO3/maturin) - ç®€åŒ–äº†æ„å»ºå’Œå‘å¸ƒæµç¨‹